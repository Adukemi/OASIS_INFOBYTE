# OASIS_INFOBYTE: Oasis Infobyte Internship
Welcome to the Oasis Infobyte Data Analytics Internship repository! Here, you will find all the tasks and projects I completed during my internship.

# About the Internship
As a Data Analytics Intern at Oasis Infobyte, my goal is to strengthen and refine my data analysis skills through hands-on tasks. Each project in this repository highlights various facets of data analysis, visualization, and reporting.

### üìÅ Projects
---
## 1: Exploratory Data Analysis (EDA) on Retail Sales P1L1

**Description**  
This project focuses on examining and interpreting retail sales data to reveal patterns, trends, and insights. The analysis includes customer demographics, purchasing habits, product popularity, and time-based buying trends to support the business in making strategic decisions.

**Key Findings**
- Sales Trends: Noted significant sales peaks and fluctuations throughout various months.
- Customer Demographics: Studied the age and gender breakdown of customers.
- Product Popularity: Investigated the most popular product categories by age and gender segments.
- Time Patterns: Identified peak shopping days and purchasing behavior based on time.

Programming Language and Libraries Used: Python, Pandas, Matplotlib, Seaborn

## 2: Customer Segmentation Analysis P2L1

**Description**  
This project focuses on customer segmentation analysis for a company. It aims to uncover distinct customer groups based on their buying patterns and demographic profiles. The insights obtained will enable the business to implement personalized marketing strategies, boost customer satisfaction, and enhance overall business outcomes.

**Process**
- Data Collection
- Data Exploration and Cleaning
- Descriptive Statistics
- Customer Segmentation: using the K-means clustering algorithm
- Visualization: boxplot, scatterplot, barplot, histogram etc.

Python: Pandas, Matplotlib, Seaborn, Scikit-Learn

## 3: Data Cleaning P3L1

**Description**
This project involves refining a dataset by resolving issues such as missing values, duplicates, inconsistent formatting, and outliers. The data cleaning process ensures data integrity by addressing gaps, eliminating redundancies, standardizing formats, and managing anomalies. The objective is to prepare the dataset for reliable analysis and modeling.

**Key Insights**
- Missing Data Resolution: Managed missing values through imputation or removal to achieve a complete dataset.
- Duplicate Handling: Detected and removed duplicate entries to maintain data quality and uniqueness.
- Format Standardization: Harmonized the structure of categorical and numeric variables for consistent and accurate analysis.
- Outlier Management: Identified and handled outliers to minimize their impact on analytical results.

## 4: Sentiment Analysis P4L1

**Description**
The primary goal is to create a sentiment analysis model that effectively categorizes the sentiment of text data, offering meaningful insights into public opinion, customer feedback, and social media trends.

**Stages**
- Sentiment Analysis: Text data was cleaned, labeled, and vectorized using TF-IDF to identify emotional tones.
- Natural Language Processing (NLP): Techniques like tokenization, lemmatization, and advanced embeddings (BERT, Word2Vec) were used to process and understand text.
- Machine Learning Models: Models like SVM, Naive Bayes, and deep learning architectures were trained and optimized through hyperparameter tuning and cross-validation.
- Feature Engineering: To improve model accuracy, key features were extracted using TF-IDF, n-Grams, sentiment scores, and embeddings.
- Data Visualization: Results were presented using confusion matrices, heatmaps, and charts to interpret model performance (accuracy, precision, recall, and F1-scores).

**Conclusion**
Systematic preprocessing, feature extraction, and advanced modeling improved sentiment classification, and the results were effectively visualized for actionable insights.

## 5: Predicting House Prices with Linear Regression P1L2

**Description**
This project aims to develop a predictive model using linear regression to forecast a numerical result based on a dataset containing key features. Linear regression is a core machine learning technique, and this project offers practical experience in constructing, assessing, and interpreting such a predictive model.

**Workflow:**
- loaded the dataset.
- Explore and clean the data (missing values, outliers, etc.).
- Selected relevant features for the model based on correlation and domain knowledge.
- Split the data into training and testing sets.
- Trained the model using linear regression.
- Evaluate the model using appropriate metrics (MSE, and R¬≤).
- Visualize the results (actual vs. predicted values, residuals).

**Conclusion**
This systematic approach allows for the creation of a robust predictive model and the effective evaluation of its performance.

## 6: Wine Quality Prediction P2L2

**Description**
The objective is to predict wine quality based on its chemical properties, providing a practical application of machine learning within viticulture. The dataset includes various chemical characteristics, such as density and acidity, which are used as features for three different classifier models.

**Process**
- Data Collection and Preparation
- Data Analysis and Preprocessing
- Model Selection and Training
- Model Evaluation
- Data Visualization
- Model Optimization

**Conclusion**
This approach helps in building a robust wine quality prediction model using machine learning classifiers.

## 7: Fraud Detection P3L2

**Description**
Fraud detection focuses on identifying and preventing fraudulent activities in financial transactions or systems. By utilizing advanced analytics and machine learning methods, fraud detection systems work to differentiate between genuine and deceptive actions. Essential elements include anomaly detection, pattern recognition, and real-time monitoring.

**Stages**
- Preprocess transaction data to prepare it for analysis.
- Applied anomaly detection to identify unusual behavior patterns.
- Train machine learning models (Logistic Regression, Decision Trees, Neural Networks) for fraud detection.
- Evaluated the model performance using appropriate metrics (accuracy, precision, recall, etc.).
- Implemented real-time monitoring systems to detect fraud as it happens.
- Ensure scalability to handle high transaction volumes efficiently.
- Deployed and monitored the system in production, continuously improving its accuracy.

**Conclusion**
This structured approach helps build a robust fraud detection system that can accurately predict and flag fraudulent activities while scaling to handle large datasets in real time.

## 8: Unveiling the Android App Market: Analyzing Google Play Store Data P4L2
